from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document

# Initialize LLM + Embeddings
llm = ChatOpenAI(temperature=0.2, model_name="gpt-3.5-turbo")
embedding_model = OpenAIEmbeddings()

def build_vector_store(text: str) -> FAISS:
    """
    Split the text and create a FAISS vector store.
    
    Args:
        text (str): Input text to embed and store.

    Returns:
        FAISS: A vector store for semantic retrieval.
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = text_splitter.create_documents([text])
    vector_store = FAISS.from_documents(chunks, embedding_model)
    return vector_store

def qa_agent(text: str, query: str) -> str:
    """
    Answer user questions using LLM + FAISS search.

    Args:
        text (str): The base document (extracted or summarized).
        query (str): The user's question.

    Returns:
        str: Answer generated by the LLM.
    """
    vector_store = build_vector_store(text)
    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 4})
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        return_source_documents=False
    )
    
    result = qa_chain.run(query)
    return result
